{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Logistic Regression \n\nwe are going to create a simple logistic regression model to classify news to either real or fake, using the same data sets, same methods of text cleaning and the same way of train_test_split.\n\nThe process is very simple and easy. We will clean and pre-process the text data, perform feature extraction using NLTK library, build and deploy a logistic regression classifier using Scikit-Learn library, and evaluate the model’s accuracy at the end.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom sklearn.linear_model import LogisticRegressionCV","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:03.187094Z","iopub.execute_input":"2022-08-16T10:46:03.187848Z","iopub.status.idle":"2022-08-16T10:46:05.448398Z","shell.execute_reply.started":"2022-08-16T10:46:03.187691Z","shell.execute_reply":"2022-08-16T10:46:05.446645Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# We load Fake.csv and True.csv\nfake_df = pd.read_csv('../input/isot-fake-news-dataset/Fake.csv')\nreal_df = pd.read_csv('../input/isot-fake-news-dataset/True.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:05.451092Z","iopub.execute_input":"2022-08-16T10:46:05.452514Z","iopub.status.idle":"2022-08-16T10:46:08.547320Z","shell.execute_reply.started":"2022-08-16T10:46:05.452453Z","shell.execute_reply":"2022-08-16T10:46:08.545968Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing\n\n","metadata":{}},{"cell_type":"code","source":"fake_df = fake_df[['title', 'text']]\nreal_df = real_df[['title', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.549629Z","iopub.execute_input":"2022-08-16T10:46:08.550208Z","iopub.status.idle":"2022-08-16T10:46:08.579160Z","shell.execute_reply.started":"2022-08-16T10:46:08.550151Z","shell.execute_reply":"2022-08-16T10:46:08.577230Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"fake_df['class'] = 0\nreal_df['class'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.584788Z","iopub.execute_input":"2022-08-16T10:46:08.586665Z","iopub.status.idle":"2022-08-16T10:46:08.595900Z","shell.execute_reply.started":"2022-08-16T10:46:08.586616Z","shell.execute_reply":"2022-08-16T10:46:08.594499Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.597545Z","iopub.execute_input":"2022-08-16T10:46:08.598001Z","iopub.status.idle":"2022-08-16T10:46:08.612783Z","shell.execute_reply.started":"2022-08-16T10:46:08.597909Z","shell.execute_reply":"2022-08-16T10:46:08.610972Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.614823Z","iopub.execute_input":"2022-08-16T10:46:08.615490Z","iopub.status.idle":"2022-08-16T10:46:08.645300Z","shell.execute_reply.started":"2022-08-16T10:46:08.615430Z","shell.execute_reply":"2022-08-16T10:46:08.643836Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.649355Z","iopub.execute_input":"2022-08-16T10:46:08.650479Z","iopub.status.idle":"2022-08-16T10:46:08.661387Z","shell.execute_reply.started":"2022-08-16T10:46:08.650419Z","shell.execute_reply":"2022-08-16T10:46:08.659025Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* Combine title & text into one column.\n* Standard text cleaning process such as lower case, remove extra spaces and url links.\n* The way we split training and testing data must be the same for deep learning model and Logistic Regression.","metadata":{}},{"cell_type":"code","source":"df['title_text'] = df['title'] + ' ' + df['text']\ndf.drop(['title', 'text'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:08.663251Z","iopub.execute_input":"2022-08-16T10:46:08.663738Z","iopub.status.idle":"2022-08-16T10:46:09.016541Z","shell.execute_reply.started":"2022-08-16T10:46:08.663690Z","shell.execute_reply":"2022-08-16T10:46:09.013896Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:09.019160Z","iopub.execute_input":"2022-08-16T10:46:09.019721Z","iopub.status.idle":"2022-08-16T10:46:09.036155Z","shell.execute_reply.started":"2022-08-16T10:46:09.019661Z","shell.execute_reply":"2022-08-16T10:46:09.034684Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"In the following pre-processing, we strip off any html tags, punctuation, and make them lower case.","metadata":{}},{"cell_type":"code","source":"def preprocessor(text):\n\n    text = re.sub('<[^>]*>', '', text)\n    text = re.sub(r'[^\\w\\s]','', text)\n    text = text.lower()\n\n    return text\n\ndf['title_text'] = df['title_text'].apply(preprocessor)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:09.043204Z","iopub.execute_input":"2022-08-16T10:46:09.045102Z","iopub.status.idle":"2022-08-16T10:46:11.885452Z","shell.execute_reply.started":"2022-08-16T10:46:09.044962Z","shell.execute_reply":"2022-08-16T10:46:11.884295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The following code combines tokenization and stemming techniques together, and then apply the techniques on “title_text”.","metadata":{}},{"cell_type":"code","source":"porter = PorterStemmer()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:11.887335Z","iopub.execute_input":"2022-08-16T10:46:11.887705Z","iopub.status.idle":"2022-08-16T10:46:11.893186Z","shell.execute_reply.started":"2022-08-16T10:46:11.887669Z","shell.execute_reply":"2022-08-16T10:46:11.892252Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF\nHere we transform “title_text” feature into TF-IDF vectors.","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(strip_accents=None,\n                        lowercase=False,\n                        preprocessor=None,\n                        tokenizer=tokenizer_porter,\n                        use_idf=True,\n                        norm='l2',\n                        smooth_idf=True\n                       )\nX = tfidf.fit_transform(df['title_text'])\ny = df['class'].values","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:11.894991Z","iopub.execute_input":"2022-08-16T10:46:11.896137Z","iopub.status.idle":"2022-08-16T10:53:47.468440Z","shell.execute_reply.started":"2022-08-16T10:46:11.896093Z","shell.execute_reply":"2022-08-16T10:53:47.466775Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* Instead of tuning C parameter manually, we can use an estimator which is LogisticRegressionCV.\n* We specify the number of cross validation folds cv=5 to tune this hyperparameter.\n* The measurement of the model is the accuracy of the classification.\n* By setting n_jobs=-1, we dedicate all the CPU cores to solve the problem.\n* We maximize the number of iterations of the optimization algorithm.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:53:47.470354Z","iopub.execute_input":"2022-08-16T10:53:47.470762Z","iopub.status.idle":"2022-08-16T10:53:47.538387Z","shell.execute_reply.started":"2022-08-16T10:53:47.470724Z","shell.execute_reply":"2022-08-16T10:53:47.537105Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:53:47.540244Z","iopub.execute_input":"2022-08-16T10:53:47.541140Z","iopub.status.idle":"2022-08-16T10:53:47.547034Z","shell.execute_reply.started":"2022-08-16T10:53:47.541097Z","shell.execute_reply":"2022-08-16T10:53:47.545634Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegressionCV(cv=5, scoring='accuracy', random_state=0, n_jobs=-1, verbose=2, max_iter=50).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:53:47.549084Z","iopub.execute_input":"2022-08-16T10:53:47.549550Z","iopub.status.idle":"2022-08-16T10:56:01.642338Z","shell.execute_reply.started":"2022-08-16T10:53:47.549504Z","shell.execute_reply":"2022-08-16T10:56:01.637555Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate the performance.","metadata":{}},{"cell_type":"code","source":"clf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:01.645877Z","iopub.execute_input":"2022-08-16T10:56:01.649807Z","iopub.status.idle":"2022-08-16T10:56:01.690831Z","shell.execute_reply.started":"2022-08-16T10:56:01.649725Z","shell.execute_reply":"2022-08-16T10:56:01.689518Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\ny_pred = clf.predict(X_test)\nprint(\"Accuracy with Logreg: {}\".format(accuracy_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:01.692555Z","iopub.execute_input":"2022-08-16T10:56:01.692999Z","iopub.status.idle":"2022-08-16T10:56:01.735050Z","shell.execute_reply.started":"2022-08-16T10:56:01.692950Z","shell.execute_reply":"2022-08-16T10:56:01.733564Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"binary_predictions = []\n\nfor i in y_pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:01.737013Z","iopub.execute_input":"2022-08-16T10:56:01.737743Z","iopub.status.idle":"2022-08-16T10:56:01.759570Z","shell.execute_reply.started":"2022-08-16T10:56:01.737690Z","shell.execute_reply":"2022-08-16T10:56:01.758400Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nmatrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(10, 6))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=15)\nax.set_ylabel('True Labels', size=15)\nax.set_title('Confusion Matrix', size=15)\nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15);","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:01.761112Z","iopub.execute_input":"2022-08-16T10:56:01.761502Z","iopub.status.idle":"2022-08-16T10:56:02.299382Z","shell.execute_reply.started":"2022-08-16T10:56:01.761468Z","shell.execute_reply":"2022-08-16T10:56:02.297904Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Step 2 : Using LSTM ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport seaborn as sns\nplt.style.use('ggplot')\nprint(\"Tensorflow version \" + tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:02.301037Z","iopub.execute_input":"2022-08-16T10:56:02.301407Z","iopub.status.idle":"2022-08-16T10:56:10.213996Z","shell.execute_reply.started":"2022-08-16T10:56:02.301373Z","shell.execute_reply":"2022-08-16T10:56:10.212643Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# loading the data again\nfake_df = pd.read_csv('../input/isot-fake-news-dataset/Fake.csv')\nreal_df = pd.read_csv('../input/isot-fake-news-dataset/True.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:10.215777Z","iopub.execute_input":"2022-08-16T10:56:10.216542Z","iopub.status.idle":"2022-08-16T10:56:11.839736Z","shell.execute_reply.started":"2022-08-16T10:56:10.216502Z","shell.execute_reply":"2022-08-16T10:56:11.838534Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"fake_df = fake_df[['title', 'text']]\nreal_df = real_df[['title', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:11.841378Z","iopub.execute_input":"2022-08-16T10:56:11.841751Z","iopub.status.idle":"2022-08-16T10:56:11.856394Z","shell.execute_reply.started":"2022-08-16T10:56:11.841719Z","shell.execute_reply":"2022-08-16T10:56:11.855101Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"fake_df['class'] = 0\nreal_df['class'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:11.857828Z","iopub.execute_input":"2022-08-16T10:56:11.858294Z","iopub.status.idle":"2022-08-16T10:56:11.867264Z","shell.execute_reply.started":"2022-08-16T10:56:11.858257Z","shell.execute_reply":"2022-08-16T10:56:11.865821Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df), color='orange')\nplt.bar('Real News', len(real_df), color='green')\nplt.title('Distribution of Fake News and Real News', size=12)\nplt.xlabel('News Type', size=12)\nplt.ylabel('# of News Articles', size=12);","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:11.869677Z","iopub.execute_input":"2022-08-16T10:56:11.871192Z","iopub.status.idle":"2022-08-16T10:56:12.073166Z","shell.execute_reply.started":"2022-08-16T10:56:11.871120Z","shell.execute_reply":"2022-08-16T10:56:12.072001Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:12.074657Z","iopub.execute_input":"2022-08-16T10:56:12.075055Z","iopub.status.idle":"2022-08-16T10:56:12.085491Z","shell.execute_reply.started":"2022-08-16T10:56:12.075020Z","shell.execute_reply":"2022-08-16T10:56:12.084300Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:12.087486Z","iopub.execute_input":"2022-08-16T10:56:12.087892Z","iopub.status.idle":"2022-08-16T10:56:12.108333Z","shell.execute_reply.started":"2022-08-16T10:56:12.087858Z","shell.execute_reply":"2022-08-16T10:56:12.107034Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df['title_text'] = df['title'] + ' ' + df['text']\ndf.drop(['title', 'text'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:12.110596Z","iopub.execute_input":"2022-08-16T10:56:12.111126Z","iopub.status.idle":"2022-08-16T10:56:12.462038Z","shell.execute_reply.started":"2022-08-16T10:56:12.111073Z","shell.execute_reply":"2022-08-16T10:56:12.460825Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X = df['title_text']\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:12.467607Z","iopub.execute_input":"2022-08-16T10:56:12.468030Z","iopub.status.idle":"2022-08-16T10:56:12.488076Z","shell.execute_reply.started":"2022-08-16T10:56:12.467994Z","shell.execute_reply":"2022-08-16T10:56:12.486174Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?://\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:12.492221Z","iopub.execute_input":"2022-08-16T10:56:12.492641Z","iopub.status.idle":"2022-08-16T10:56:28.559858Z","shell.execute_reply.started":"2022-08-16T10:56:12.492606Z","shell.execute_reply":"2022-08-16T10:56:28.558332Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### We put the parameters at the top like this to make it easier to change and edit.\n","metadata":{}},{"cell_type":"code","source":"vocab_size = 10000\nembedding_dim = 64\nmax_length = 256\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:28.561877Z","iopub.execute_input":"2022-08-16T10:56:28.562447Z","iopub.status.idle":"2022-08-16T10:56:28.569490Z","shell.execute_reply.started":"2022-08-16T10:56:28.562390Z","shell.execute_reply":"2022-08-16T10:56:28.567897Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization\n* Tokenizer does all the heavy lifting for us. In our articles (aka title + text) that it was tokenizing, it will take 10,000 most common words. oov_tok is to put a special value in where an unseen word is encountered. This means I want “OOV” in bracket to be used to for words that are not in the word index. fit_on_text will go through all the text and create dictionary.\n* After tokenization, the next step is to turn those tokens into lists of sequence.\n* When we train neural networks for NLP, we need sequences to be in the same size, that’s why we use padding. Our max_length is 256, so we use pad_sequence to make all of our articles (aka title + text) the same length which is 256.\n* In addition, there are padding type and truncating type, we set both of them “post”., meaning for example, if one article at 200 in length, we padded to 256, and we padded at the end, add 56 zeros.","metadata":{}},{"cell_type":"code","source":"## tokenizer = Tokenizer(num_words=max_vocab)\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding=padding_type, truncating=trunc_type, maxlen=max_length)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=padding_type, truncating=trunc_type, maxlen=max_length)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:28.571372Z","iopub.execute_input":"2022-08-16T10:56:28.572083Z","iopub.status.idle":"2022-08-16T10:56:53.595041Z","shell.execute_reply.started":"2022-08-16T10:56:28.572031Z","shell.execute_reply":"2022-08-16T10:56:53.593413Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Building the Model\n* Now we can implement LSTM. Here is my code that I build a tf.keras.Sequential model and start with an embedding layer.\n* An embedding layer stores one vector per word. When called, it converts the sequences of word indices into sequences of vectors. After training, words with similar meanings often have the similar vectors.\n* Next is how to implement LSTM in code. The Bidirectional wrapper is used with a LSTM layer, this propagates the input forwards and backwards through the LSTM layer and then concatenates the outputs. This helps LSTM to learn long term dependencies. We then fit it to a dense neural network to do classification.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:56:53.596647Z","iopub.execute_input":"2022-08-16T10:56:53.597047Z","iopub.status.idle":"2022-08-16T10:56:55.136665Z","shell.execute_reply.started":"2022-08-16T10:56:53.597014Z","shell.execute_reply":"2022-08-16T10:56:55.135345Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# We are using early stop, which stops when the validation loss no longer improves.\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1,verbose=1, batch_size=30, shuffle=True, callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(10,6))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=15)\nplt.xlabel('Epochs', size=15)\nplt.ylabel('Loss', size=15)\nplt.legend(prop={'size': 15})\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=15)\nplt.xlabel('Epochs', size=15)\nplt.ylabel('Accuracy', size=15)\nplt.legend(prop={'size': 15})\nplt.ylim((0.5,1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(10, 6))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=15)\nax.set_ylabel('True Labels', size=15)\nax.set_title('Confusion Matrix', size=15)\nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}